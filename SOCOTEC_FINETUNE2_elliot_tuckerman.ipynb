{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "authorship_tag": "ABX9TyM/DoIua9JpxpsJZbqr3Llk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "a070edda2575452c84746b1f61168ae0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e7dd03c2be6340a9bf0a39f9a02b26a0",
              "IPY_MODEL_7cb3646563754706a4ae3a63dd07baaf",
              "IPY_MODEL_45cf816cd1cc4fef9f9aeb46f3d44096"
            ],
            "layout": "IPY_MODEL_54377eee90cf405b9938156ad8f3c6a8"
          }
        },
        "e7dd03c2be6340a9bf0a39f9a02b26a0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_df7cccb89b8040a9bc149d7f6447e86d",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_ae547844bcb148c59ad83df7736f82db",
            "value": "Map:â€‡100%"
          }
        },
        "7cb3646563754706a4ae3a63dd07baaf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7fa26f5ae6c346fa8468d4134a7e360c",
            "max": 411,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_bccf37557fb845a69378922f42d96cc0",
            "value": 411
          }
        },
        "45cf816cd1cc4fef9f9aeb46f3d44096": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_64c1d38d384a4b0891e4cfdbdec4f0fc",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_3e75790a183c4459a760bda69d68a2b0",
            "value": "â€‡411/411â€‡[00:00&lt;00:00,â€‡11120.88â€‡examples/s]"
          }
        },
        "54377eee90cf405b9938156ad8f3c6a8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "df7cccb89b8040a9bc149d7f6447e86d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ae547844bcb148c59ad83df7736f82db": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7fa26f5ae6c346fa8468d4134a7e360c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bccf37557fb845a69378922f42d96cc0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "64c1d38d384a4b0891e4cfdbdec4f0fc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3e75790a183c4459a760bda69d68a2b0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d57777938940485f9922644d0c67c939": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a29d1e0197da45319ae75ca1375fa71b",
              "IPY_MODEL_731c8f6c69ed4e09a54443fb540e8bb0",
              "IPY_MODEL_d0cdd206c42c49f59af1900d4339f4ea"
            ],
            "layout": "IPY_MODEL_62790dc0480b420294068613a6a5e7cb"
          }
        },
        "a29d1e0197da45319ae75ca1375fa71b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_24999cea062d47afa8e53c92aca2fa1d",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_e18c1ece115a4abcb9ffdf3a3ce6dac8",
            "value": "Mapâ€‡(num_proc=2):â€‡100%"
          }
        },
        "731c8f6c69ed4e09a54443fb540e8bb0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_511d5d3d0cc74bc3966700b22e2bf899",
            "max": 411,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ca270123e8fe4a53ad003bb457dfd607",
            "value": 411
          }
        },
        "d0cdd206c42c49f59af1900d4339f4ea": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_add0a18a09b84fec96662258a98069a2",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_b8dc9547e2954e1baae6ef2d5d20bf32",
            "value": "â€‡411/411â€‡[00:00&lt;00:00,â€‡638.91â€‡examples/s]"
          }
        },
        "62790dc0480b420294068613a6a5e7cb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "24999cea062d47afa8e53c92aca2fa1d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e18c1ece115a4abcb9ffdf3a3ce6dac8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "511d5d3d0cc74bc3966700b22e2bf899": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ca270123e8fe4a53ad003bb457dfd607": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "add0a18a09b84fec96662258a98069a2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b8dc9547e2954e1baae6ef2d5d20bf32": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/etuckerman/SOCOTEC/blob/main/SOCOTEC_FINETUNE2_elliot_tuckerman.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "#install unsloth, xformers (for flash attn) and other pckgs\n",
        "!pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
        "!pip install --no-deps xformers trl peft accelerate bitsandbytes"
      ],
      "metadata": {
        "id": "wmSUQx61W0eh"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yTKjDs0jWkXZ",
        "outputId": "bc47cb45-83b4-4190-8511-6fcc5ab67568"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
            "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n",
            "==((====))==  Unsloth 2024.12.4: Fast Mistral patching. Transformers:4.46.3.\n",
            "   \\\\   /|    GPU: NVIDIA A100-SXM4-40GB. Max memory: 39.564 GB. Platform: Linux.\n",
            "O^O/ \\_/ \\    Torch: 2.5.1+cu121. CUDA: 8.0. CUDA Toolkit: 12.1. Triton: 3.1.0\n",
            "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.28.post3. FA2 = False]\n",
            " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Unsloth: Dropout = 0 is supported for fast patching. You are using dropout = 0.1.\n",
            "Unsloth will patch all other layers, except LoRA matrices, causing a performance hit.\n",
            "Unsloth 2024.12.4 patched 32 layers with 0 QKV layers, 0 O layers and 0 MLP layers.\n"
          ]
        }
      ],
      "source": [
        "from unsloth import FastLanguageModel\n",
        "import pandas as pd\n",
        "import torch\n",
        "from datasets import Dataset\n",
        "from transformers import TrainingArguments\n",
        "from trl import SFTTrainer\n",
        "from unsloth import is_bfloat16_supported\n",
        "\n",
        "# Constants\n",
        "MODEL_NAME = \"unsloth/mistral-7b-v0.3\"\n",
        "MAX_SEQ_LENGTH = 2048\n",
        "LOAD_IN_4BIT = True\n",
        "BATCH_SIZE = 2\n",
        "GRAD_ACCUMULATION = 4\n",
        "LEARNING_RATE = 2e-4\n",
        "TRAIN_STEPS = 100  # Optimized training steps\n",
        "OUTPUT_DIR = \"outputs\"\n",
        "SEED = 3407\n",
        "\n",
        "# Load model and tokenizer\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name=MODEL_NAME,\n",
        "    max_seq_length=MAX_SEQ_LENGTH,\n",
        "    dtype=None,\n",
        "    load_in_4bit=LOAD_IN_4BIT\n",
        ")\n",
        "\n",
        "# Apply LoRA Adapters\n",
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r=16,\n",
        "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
        "    lora_alpha=16,\n",
        "    lora_dropout=0.1,  # Small dropout for regularization\n",
        "    bias=\"none\",\n",
        "    use_gradient_checkpointing=\"unsloth\",\n",
        "    random_state=SEED,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import itertools\n",
        "import csv\n",
        "\n",
        "def generate_multi_function_examples():\n",
        "    # Numbers to use in combinations\n",
        "    numbers = list(range(1, 21))\n",
        "\n",
        "    # Function combinations and their natural language templates\n",
        "    multi_function_examples = [\n",
        "        # Square and Add combinations\n",
        "        {\n",
        "            'prompt': f\"What is the square of {n1} added to {n2}?\",\n",
        "            'function_call': f\"add(square({n1}), {n2})\"\n",
        "        } for n1, n2 in itertools.combinations(range(1, 11), 2)\n",
        "    ] + [\n",
        "        # Cube and Add combinations\n",
        "        {\n",
        "            'prompt': f\"Add the cube of {n1} to {n2}\",\n",
        "            'function_call': f\"add(cube({n1}), {n2})\"\n",
        "        } for n1, n2 in itertools.combinations(range(1, 11), 2)\n",
        "    ] + [\n",
        "        # Add cubed results\n",
        "        {\n",
        "            'prompt': f\"What do you get when you add the cube of {n1} and the cube of {n2}?\",\n",
        "            'function_call': f\"add(cube({n1}), cube({n2}))\"\n",
        "        } for n1, n2 in itertools.combinations(range(1, 11), 2)\n",
        "    ] + [\n",
        "        # Add squared results\n",
        "        {\n",
        "            'prompt': f\"Sum the squares of {n1} and {n2}\",\n",
        "            'function_call': f\"add(square({n1}), square({n2}))\"\n",
        "        } for n1, n2 in itertools.combinations(range(1, 11), 2)\n",
        "    ] + [\n",
        "        # Complex combinations\n",
        "        {\n",
        "            'prompt': f\"What is the cube of {n1} plus the square of {n2}?\",\n",
        "            'function_call': f\"add(cube({n1}), square({n2}))\"\n",
        "        } for n1, n2 in itertools.combinations(range(1, 11), 2)\n",
        "    ] + [\n",
        "        # Greetings with numbers\n",
        "        {\n",
        "            'prompt': f\"Greet {n1} friends. The first friend is Alice.\",\n",
        "            'function_call': f'greet(\"Alice\")'\n",
        "        } for n1 in range(1, 6)\n",
        "    ] + [\n",
        "        # Name-based examples\n",
        "        {\n",
        "            'prompt': f\"Multiply {n1} by itself then add {n2}\",\n",
        "            'function_call': f\"add(square({n1}), {n2})\"\n",
        "        } for n1, n2 in itertools.combinations(range(1, 11), 2)\n",
        "    ]\n",
        "\n",
        "    return multi_function_examples\n",
        "\n",
        "def append_to_csv(filename, new_examples):\n",
        "    # Read existing CSV to avoid duplicates\n",
        "    existing_prompts = set()\n",
        "    with open(filename, 'r', newline='') as csvfile:\n",
        "        reader = csv.DictReader(csvfile)\n",
        "        existing_prompts = {row['Prompt'] for row in reader}\n",
        "\n",
        "    # Append new unique examples\n",
        "    with open(filename, 'a', newline='') as csvfile:\n",
        "        writer = csv.writer(csvfile)\n",
        "        for example in new_examples:\n",
        "            if example['prompt'] not in existing_prompts:\n",
        "                writer.writerow([example['prompt'], example['function_call']])\n",
        "                existing_prompts.add(example['prompt'])\n",
        "\n",
        "# Generate and append multi-function examples\n",
        "new_examples = generate_multi_function_examples()\n",
        "append_to_csv('SOCOTEC_DATASET.csv', new_examples)\n",
        "\n",
        "print(f\"Added {len(new_examples)} new multi-function examples to the dataset.\")\n",
        "# Load and process dataset\n",
        "dataset = pd.read_csv(\"SOCOTEC_DATASET.csv\")\n",
        "EOS_TOKEN = tokenizer.eos_token\n",
        "\n",
        "# Modify your dataset to ensure proper function calls (not just the prompt)\n",
        "# Assuming `dataset` is a pandas dataframe that has been pre-loaded with prompts and function calls\n",
        "def format_data(examples):\n",
        "    # Ensure function calls are correctly formatted\n",
        "    function_call = examples[\"Function Call\"]\n",
        "    return {\"text\": f\"Question: {examples['Prompt']} \\nFunction Call: {function_call}\\n\"}\n",
        "\n",
        "# Preprocess dataset (if you have a new dataset with proper function calls)\n",
        "dataset = Dataset.from_pandas(dataset)  # Assuming the dataset is preloaded\n",
        "dataset = dataset.map(format_data)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66,
          "referenced_widgets": [
            "a070edda2575452c84746b1f61168ae0",
            "e7dd03c2be6340a9bf0a39f9a02b26a0",
            "7cb3646563754706a4ae3a63dd07baaf",
            "45cf816cd1cc4fef9f9aeb46f3d44096",
            "54377eee90cf405b9938156ad8f3c6a8",
            "df7cccb89b8040a9bc149d7f6447e86d",
            "ae547844bcb148c59ad83df7736f82db",
            "7fa26f5ae6c346fa8468d4134a7e360c",
            "bccf37557fb845a69378922f42d96cc0",
            "64c1d38d384a4b0891e4cfdbdec4f0fc",
            "3e75790a183c4459a760bda69d68a2b0"
          ]
        },
        "id": "cW2r5OLajYx5",
        "outputId": "30e37938-5558-4ba0-8182-78e00b7928d9"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Added 275 new multi-function examples to the dataset.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/411 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a070edda2575452c84746b1f61168ae0"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert the dataset to a Pandas DataFrame\n",
        "dataset_df = dataset.to_pandas()\n",
        "\n",
        "# Display the first few rows of the dataset (default is 5 rows)\n",
        "print(dataset_df.head(20))\n"
      ],
      "metadata": {
        "id": "z4PP1b1Zysk-",
        "outputId": "b9a3879b-ff38-4e56-ba7e-acef19fe6817",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                       Prompt Function Call  \\\n",
            "0                     What's the square of 6?     square(6)   \n",
            "1                   Calculate the square of 7     square(7)   \n",
            "2                 Find the squared value of 5     square(5)   \n",
            "3          What do you get when you square 4?     square(4)   \n",
            "4                     Compute the square of 3     square(3)   \n",
            "5                    Can you square 8 for me?     square(8)   \n",
            "6            What is the result of 9 squared?     square(9)   \n",
            "7                                    Square 2     square(2)   \n",
            "8                    Tell me the square of 10    square(10)   \n",
            "9             What number is 11 when squared?    square(11)   \n",
            "10           Give me the squared result of 12    square(12)   \n",
            "11                 Determine the square of 13    square(13)   \n",
            "12                    How much is 14 squared?    square(14)   \n",
            "13                  Square root of 16 squared     square(4)   \n",
            "14                    I want the square of 15    square(15)   \n",
            "15                         Squared value of 7     square(7)   \n",
            "16  What does 6 look like when you square it?     square(6)   \n",
            "17                        Double squared of 3     square(2)   \n",
            "18                     The square value for 9     square(9)   \n",
            "19                                  Square 17    square(17)   \n",
            "\n",
            "                                                 text  \n",
            "0   Question: What's the square of 6? \\nFunction C...  \n",
            "1   Question: Calculate the square of 7 \\nFunction...  \n",
            "2   Question: Find the squared value of 5 \\nFuncti...  \n",
            "3   Question: What do you get when you square 4? \\...  \n",
            "4   Question: Compute the square of 3 \\nFunction C...  \n",
            "5   Question: Can you square 8 for me? \\nFunction ...  \n",
            "6   Question: What is the result of 9 squared? \\nF...  \n",
            "7     Question: Square 2 \\nFunction Call: square(2)\\n  \n",
            "8   Question: Tell me the square of 10 \\nFunction ...  \n",
            "9   Question: What number is 11 when squared? \\nFu...  \n",
            "10  Question: Give me the squared result of 12 \\nF...  \n",
            "11  Question: Determine the square of 13 \\nFunctio...  \n",
            "12  Question: How much is 14 squared? \\nFunction C...  \n",
            "13  Question: Square root of 16 squared \\nFunction...  \n",
            "14  Question: I want the square of 15 \\nFunction C...  \n",
            "15  Question: Squared value of 7 \\nFunction Call: ...  \n",
            "16  Question: What does 6 look like when you squar...  \n",
            "17  Question: Double squared of 3 \\nFunction Call:...  \n",
            "18  Question: The square value for 9 \\nFunction Ca...  \n",
            "19  Question: Square 17 \\nFunction Call: square(17)\\n  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Training arguments\n",
        "training_args = TrainingArguments(\n",
        "    per_device_train_batch_size=BATCH_SIZE,\n",
        "    gradient_accumulation_steps=GRAD_ACCUMULATION,\n",
        "    warmup_steps=5,\n",
        "    max_steps=TRAIN_STEPS,\n",
        "    learning_rate=LEARNING_RATE,\n",
        "    fp16=not is_bfloat16_supported(),\n",
        "    bf16=is_bfloat16_supported(),\n",
        "    logging_steps=10,\n",
        "    optim=\"adamw_8bit\",\n",
        "    weight_decay=0.01,\n",
        "    lr_scheduler_type=\"linear\",\n",
        "    seed=SEED,\n",
        "    output_dir=OUTPUT_DIR,\n",
        "    report_to=\"none\",\n",
        ")\n",
        "\n",
        "# Initialize Trainer\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    train_dataset=dataset,\n",
        "    dataset_text_field=\"text\",\n",
        "    max_seq_length=MAX_SEQ_LENGTH,\n",
        "    dataset_num_proc=2,\n",
        "    packing=False,\n",
        "    args=training_args,\n",
        ")\n",
        "\n",
        "# Train model\n",
        "trainer_stats = trainer.train()\n",
        "\n",
        "# Print memory usage and training time\n",
        "gpu_stats = torch.cuda.get_device_properties(0)\n",
        "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
        "print(f\"GPU: {gpu_stats.name}\")\n",
        "print(f\"Peak memory usage: {used_memory} GB\")\n",
        "print(f\"Training runtime: {trainer_stats.metrics['train_runtime']} seconds\")\n",
        "\n",
        "# Inference\n",
        "FastLanguageModel.for_inference(model)\n",
        "input_text = \"What is 2 plus 3?\"\n",
        "inputs = tokenizer([input_text], return_tensors=\"pt\").to(\"cuda\")\n",
        "outputs = model.generate(**inputs, max_new_tokens=16, use_cache=True)\n",
        "result = tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]\n",
        "print(f\"Generated Function Call: {result}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 646,
          "referenced_widgets": [
            "d57777938940485f9922644d0c67c939",
            "a29d1e0197da45319ae75ca1375fa71b",
            "731c8f6c69ed4e09a54443fb540e8bb0",
            "d0cdd206c42c49f59af1900d4339f4ea",
            "62790dc0480b420294068613a6a5e7cb",
            "24999cea062d47afa8e53c92aca2fa1d",
            "e18c1ece115a4abcb9ffdf3a3ce6dac8",
            "511d5d3d0cc74bc3966700b22e2bf899",
            "ca270123e8fe4a53ad003bb457dfd607",
            "add0a18a09b84fec96662258a98069a2",
            "b8dc9547e2954e1baae6ef2d5d20bf32"
          ]
        },
        "id": "qvWO2vkIjZeU",
        "outputId": "b41cd9c3-b91d-4e2a-8cea-92cf27ea5df3"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map (num_proc=2):   0%|          | 0/411 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d57777938940485f9922644d0c67c939"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "max_steps is given, it will override any value given in num_train_epochs\n",
            "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
            "   \\\\   /|    Num examples = 411 | Num Epochs = 2\n",
            "O^O/ \\_/ \\    Batch size per device = 2 | Gradient Accumulation steps = 4\n",
            "\\        /    Total batch size = 8 | Total steps = 100\n",
            " \"-____-\"     Number of trainable parameters = 41,943,040\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [100/100 03:36, Epoch 1/2]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>1.938200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>0.580100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>0.423800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>0.329100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>0.356700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60</td>\n",
              "      <td>0.336800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>70</td>\n",
              "      <td>0.345200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>80</td>\n",
              "      <td>0.299000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>90</td>\n",
              "      <td>0.293900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>0.285200</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU: NVIDIA A100-SXM4-40GB\n",
            "Peak memory usage: 5.104 GB\n",
            "Training runtime: 222.035 seconds\n",
            "Generated Function Call: What is 2 plus 3? \n",
            "Function Call: add(2, 3)\n",
            "\n",
            "What is\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Inference Adjustments: Make the model generate structured outputs (function calls)\n",
        "def generate_function_call(input_text):\n",
        "    FastLanguageModel.for_inference(model)\n",
        "    inputs = tokenizer([input_text], return_tensors=\"pt\").to(\"cuda\")\n",
        "    outputs = model.generate(**inputs, max_new_tokens=16, use_cache=True)\n",
        "    result = tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]\n",
        "\n",
        "    # Post-process the result to make sure itâ€™s a valid function call\n",
        "    #print(f\"Generated Output: {result}\")\n",
        "    return result\n",
        "\n",
        "# Example inference (adjust input for testing)\n",
        "input_text = \"What is 2 plus 3?\"\n",
        "res = generate_function_call(input_text)\n",
        "print(f\"[{input_text}] \\n [{res}] \")\n",
        "\n",
        "input_text = \"What is two squared plus 6?\"\n",
        "res = generate_function_call(input_text)\n",
        "print(f\"[{input_text}] \\n [{res}] \")\n",
        "\n",
        "input_text = \"162 squared plus 1902\"\n",
        "res = generate_function_call(input_text)\n",
        "print(f\"[{input_text}] \\n [{res}] \")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wi_NMbGzoYz5",
        "outputId": "e2a93d25-d3c4-4810-dcff-71b641aefd0e"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[What is 2 plus 3?] \n",
            " [What is 2 plus 3? \n",
            "Function Call: add(2, 3)\n",
            "\n",
            "What is] \n",
            "[What is two squared plus 6?] \n",
            " [What is two squared plus 6? \n",
            "Function Call: add(square(2), 6)\n",
            "\n",
            "] \n",
            "[162 squared plus 1902] \n",
            " [162 squared plus 1902 \n",
            "Function Call: add(square(162), 19] \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Inference\n",
        "FastLanguageModel.for_inference(model)\n",
        "input_text = \"increase 6 by 2\"\n",
        "inputs = tokenizer([input_text], return_tensors=\"pt\").to(\"cuda\")\n",
        "outputs = model.generate(**inputs, max_new_tokens=16, use_cache=True)\n",
        "result = tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]\n",
        "print(f\"Generated Function Call: {result}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tEjUokkSWyDL",
        "outputId": "caeffdb0-de48-4798-d173-c3f1d1321e70"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated Function Call: increase 6 by 2 \n",
            "Function Call: add(6, 2)\n",
            "\n",
            "add \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Inference\n",
        "FastLanguageModel.for_inference(model)\n",
        "input_text = \"say wassup to pete\"\n",
        "inputs = tokenizer([input_text], return_tensors=\"pt\").to(\"cuda\")\n",
        "outputs = model.generate(**inputs, max_new_tokens=16, use_cache=True)\n",
        "result = tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]\n",
        "print(f\"Generated Function Call: {result}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QD4OCRnZYFuI",
        "outputId": "e1b0bbc1-e4d2-4cc4-ca84-5bcb6d2d563e"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated Function Call: say wassup to pete \n",
            "Function Call: greet(\"Pete\")\n",
            "\n",
            "say wassup\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Inference\n",
        "FastLanguageModel.for_inference(model)\n",
        "input_text = \"increase 3 by negative 2\"\n",
        "inputs = tokenizer([input_text], return_tensors=\"pt\").to(\"cuda\")\n",
        "outputs = model.generate(**inputs, max_new_tokens=16, use_cache=True)\n",
        "result = tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]\n",
        "print(f\"Generated Function Call: {result}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Rs6M-r4Yh5q",
        "outputId": "d69c68ec-9b0b-4423-f689-b468bed02f3c"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated Function Call: increase 3 by negative 2 \n",
            "Function Call: add(3, -2)\n",
            "\n",
            "add \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Inference\n",
        "FastLanguageModel.for_inference(model)\n",
        "input_text = \"add 3 to 2 cubed\"\n",
        "inputs = tokenizer([input_text], return_tensors=\"pt\").to(\"cuda\")\n",
        "outputs = model.generate(**inputs, max_new_tokens=16, use_cache=True)\n",
        "result = tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]\n",
        "print(f\"Generated Function Call: {result}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rQGQYXohYnZp",
        "outputId": "4ffcb4d0-2435-4ad0-c985-85db5b00d1f5"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated Function Call: add 3 to 2 cubed \n",
            "Function Call: cube(5)\n",
            "\n",
            "What is the c\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from unsloth import FastLanguageModel\n",
        "from transformers import AutoTokenizer\n",
        "import torch\n",
        "\n",
        "\n",
        "# Inference Adjustments: Make the model generate structured outputs (function calls)\n",
        "def generate_function_call(input_text):\n",
        "    # Add a special instruction to ensure model only returns function call\n",
        "    instruction = \"Return only the function call, no additional text.\"\n",
        "\n",
        "    # Prepare the input by adding instruction\n",
        "    input_with_instruction = f\"{instruction} {input_text}\"\n",
        "\n",
        "    # Tokenize the input\n",
        "    inputs = tokenizer([input_with_instruction], return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "    # Generate output from the model\n",
        "    outputs = model.generate(**inputs, max_new_tokens=16, use_cache=True)\n",
        "\n",
        "    # Decode the output and clean up (strip any unnecessary text after the function call)\n",
        "    result = tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]\n",
        "\n",
        "    # Ensure that the result only contains the function call (strip extra text if present)\n",
        "    if \"(\" in result:  # Ensuring a valid function call is present\n",
        "        return result\n",
        "    else:\n",
        "        return \"Invalid function call\"\n",
        "\n",
        "# Example inference (adjust input for testing)\n",
        "input_texts = [\n",
        "    \"What is 2 plus 3?\",\n",
        "    \"What is two squared plus 6?\",\n",
        "    \"162 squared plus 1902\",\n",
        "    \"increase 6 by 2\",\n",
        "    \"say wassup to pete\",\n",
        "    \"increase 3 by negative 2\",\n",
        "    \"add 3 to 2 cubed\"\n",
        "]\n",
        "\n",
        "# Test the model's response for each input\n",
        "for input_text in input_texts:\n",
        "    res = generate_function_call(input_text)\n",
        "    print(f\"Input: [{input_text}] \\nGenerated Function Call: [{res}]\")\n"
      ],
      "metadata": {
        "id": "ChatfLU_YqVo",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 315
        },
        "outputId": "51f62b4e-fac1-4f5c-fa61-c339bfcc1444"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'tuple' object has no attribute 'generate'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-50d9b616bff4>\u001b[0m in \u001b[0;36m<cell line: 41>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;31m# Test the model's response for each input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0minput_text\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minput_texts\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m     \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_function_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Input: [{input_text}] \\nGenerated Function Call: [{res}]\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-15-50d9b616bff4>\u001b[0m in \u001b[0;36mgenerate_function_call\u001b[0;34m(input_text)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;31m# Generate output from the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_new_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_cache\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;31m# Decode the output and clean up (strip any unnecessary text after the function call)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'generate'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nQkV2-VYyfkt"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}