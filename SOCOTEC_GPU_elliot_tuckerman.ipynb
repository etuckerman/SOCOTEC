{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "authorship_tag": "ABX9TyNEWbUKa+t2hgrd+fq7ws6M",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "50e96932174d40238701e042202bd9be": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d845fe2d937a41439f5a76a8afced2bd",
              "IPY_MODEL_151c9bef718c496495278046fef0da42",
              "IPY_MODEL_a33783aa24904bfaba4275a8e9a14310"
            ],
            "layout": "IPY_MODEL_2448456c676149fca6c4d0a2b6bb6dd1"
          }
        },
        "d845fe2d937a41439f5a76a8afced2bd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_90a51a28dd8d43e5bbc24fe135433473",
            "placeholder": "​",
            "style": "IPY_MODEL_e7afced176dd4aa1937146fc966ba8d2",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "151c9bef718c496495278046fef0da42": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_475917b794cc4ba588f0f3d59c22ef4c",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_524dc35d9ea24c67a58b55c731503631",
            "value": 2
          }
        },
        "a33783aa24904bfaba4275a8e9a14310": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8e39d4d42d7b49309865152aac3560ed",
            "placeholder": "​",
            "style": "IPY_MODEL_47bc2599a761449e9967a795a448eefb",
            "value": " 2/2 [00:10&lt;00:00,  4.85s/it]"
          }
        },
        "2448456c676149fca6c4d0a2b6bb6dd1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "90a51a28dd8d43e5bbc24fe135433473": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e7afced176dd4aa1937146fc966ba8d2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "475917b794cc4ba588f0f3d59c22ef4c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "524dc35d9ea24c67a58b55c731503631": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "8e39d4d42d7b49309865152aac3560ed": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "47bc2599a761449e9967a795a448eefb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/etuckerman/SOCOTEC/blob/main/SOCOTEC_GPU_elliot_tuckerman.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!huggingface-cli login"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XkCUTOUXqJRq",
        "outputId": "d2e88401-896a-4b30-8b0d-453fcf7e323a"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "    _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n",
            "    _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n",
            "    _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n",
            "    _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n",
            "    _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n",
            "\n",
            "    A token is already saved on your machine. Run `huggingface-cli whoami` to get more information or `huggingface-cli logout` if you want to log out.\n",
            "    Setting a new token will erase the existing one.\n",
            "    To log in, `huggingface_hub` requires a token generated from https://huggingface.co/settings/tokens .\n",
            "Enter your token (input will not be visible): \n",
            "Add token as git credential? (Y/n) n\n",
            "Token is valid (permission: read).\n",
            "The token `SOCOTEC` has been saved to /root/.cache/huggingface/stored_tokens\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful.\n",
            "The current active token is: `SOCOTEC`\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check for GPU availability\n",
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nUnzndM9py19",
        "outputId": "cd40fb50-e6b1-4521-efa6-794b4e725185"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mon Dec  9 18:29:59 2024       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  NVIDIA A100-SXM4-40GB          Off | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   32C    P0              53W / 400W |  10257MiB / 40960MiB |      0%      Default |\n",
            "|                                         |                      |             Disabled |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "+---------------------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install required libraries\n",
        "!pip install -q transformers torch accelerate datasets openai\n",
        "!pip install -q bitsandbytes\n",
        "\n",
        "import torch\n",
        "import transformers\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
        "import re"
      ],
      "metadata": {
        "id": "Y2O4YZQCp3wo"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Implement function library as specified\n",
        "def add(a, b): return a + b\n",
        "def square(a): return a ** 2\n",
        "def cube(a): return a ** 3\n",
        "def greet(name): return f\"Hello, {name}!\"\n",
        "\n",
        "# Function mapping dictionary\n",
        "FUNCTION_MAPPING = {\n",
        "    'add': add,\n",
        "    'square': square,\n",
        "    'cube': cube,\n",
        "    'greet': greet\n",
        "}"
      ],
      "metadata": {
        "id": "1DaCRGrqp5Ql"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create synthetic training dataset\n",
        "training_data = [\n",
        "    {\"input\": \"What is the sum of 1 and 2?\", \"output\": \"add(1, 2)\"}\n",
        "]\n",
        "\n",
        "# Additional diverse prompts\n",
        "# Comprehensive test cases covering all functions\n",
        "test_cases = training_data + [\n",
        "    # Organized test cases with diverse scenarios\n",
        "    # Basic addition scenarios\n",
        "    {\"input\": \"What is the sum of 7 and 3?\", \"output\": \"add(7, 3)\"},\n",
        "    {\"input\": \"Add 10 and 15\", \"output\": \"add(10, 15)\"},\n",
        "    {\"input\": \"How much is 20 plus 22?\", \"output\": \"add(20, 22)\"},\n",
        "    {\"input\": \"Add 50 and 30\", \"output\": \"add(50, 30)\"},\n",
        "    {\"input\": \"What is 18 plus 27?\", \"output\": \"add(18, 27)\"},\n",
        "\n",
        "    # Square function scenarios\n",
        "    {\"input\": \"Calculate the square of 5\", \"output\": \"square(5)\"},\n",
        "    {\"input\": \"Square of 6\", \"output\": \"square(6)\"},\n",
        "    {\"input\": \"Square 7\", \"output\": \"square(7)\"},\n",
        "    {\"input\": \"Square of 4\", \"output\": \"square(4)\"},\n",
        "    {\"input\": \"Square of 8\", \"output\": \"square(8)\"},\n",
        "\n",
        "    # Cube function scenarios\n",
        "    {\"input\": \"What's the cube of 2?\", \"output\": \"cube(2)\"},\n",
        "    {\"input\": \"Cube the number 4\", \"output\": \"cube(4)\"},\n",
        "    {\"input\": \"Cube of 3\", \"output\": \"cube(3)\"},\n",
        "    {\"input\": \"Cube 5\", \"output\": \"cube(5)\"},\n",
        "    {\"input\": \"Cube of 6\", \"output\": \"cube(6)\"},\n",
        "\n",
        "    # Greeting scenarios\n",
        "    {\"input\": \"Greet John\", \"output\": \"greet('John')\"},\n",
        "    {\"input\": \"Say hello to Sarah\", \"output\": \"greet('Sarah')\"},\n",
        "    {\"input\": \"Greet Emily\", \"output\": \"greet('Emily')\"},\n",
        "    {\"input\": \"Greet Michael\", \"output\": \"greet('Michael')\"},\n",
        "\n",
        "    # BONUS: Multi-function scenarios (to demonstrate potential)\n",
        "    {\"input\": \"Add the square of 3 to 10\", \"output\": \"add(square(3), 10)\"},\n",
        "    {\"input\": \"Greet the person who got the cube of 2\", \"output\": \"greet('cube(2)')\"}\n",
        "]"
      ],
      "metadata": {
        "id": "OoHsr-2Sp6VD"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def format_prompt(input_text):\n",
        "    return f\"\"\"You are an expert at converting natural language to precise Python function calls using ONLY these predefined functions:\n",
        "- add(a, b): Returns the sum of a and b\n",
        "- square(a): Returns a squared\n",
        "- cube(a): Returns a cubed\n",
        "- greet(name): Returns a greeting for the given name\n",
        "\n",
        "IMPORTANT: ONLY use these exact function names and signatures.\n",
        "\n",
        "Convert the following input to EXACTLY the correct function call:\n",
        "Input: {input_text}\n",
        "Correct Function Call: \"\"\"\n",
        "\n",
        "def generate_function_call(model, tokenizer, input_text, max_length=150):\n",
        "    prompt = format_prompt(input_text)\n",
        "\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
        "    outputs = model.generate(\n",
        "        **inputs,\n",
        "        max_length=max_length,\n",
        "        num_return_sequences=1,\n",
        "        do_sample=True,\n",
        "        temperature=0.1  # Very low temperature for more precise output\n",
        "    )\n",
        "\n",
        "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "    # More robust extraction of function call\n",
        "    function_call_match = re.search(r'Correct Function Call:\\s*(.+)', generated_text)\n",
        "\n",
        "    if function_call_match:\n",
        "        function_call = function_call_match.group(1).strip()\n",
        "\n",
        "        # Standardize quotes and prevent generic Python operations\n",
        "        function_call = function_call.replace('\"', \"'\")\n",
        "\n",
        "        # Prevent generic print or math operations\n",
        "        if not re.match(r'^(add|square|cube|greet)\\(', function_call):\n",
        "            return None\n",
        "\n",
        "        return function_call\n",
        "\n",
        "    return None\n",
        "\n",
        "def execute_function_call(function_call):\n",
        "    try:\n",
        "        match = re.match(r'(\\w+)\\((.*?)\\)', function_call)\n",
        "        if not match:\n",
        "            return \"Invalid function call format\"\n",
        "\n",
        "        func_name, args_str = match.groups()\n",
        "\n",
        "        # More robust argument parsing\n",
        "        args = []\n",
        "        for arg in re.findall(r\"'[^']*'|\\\"[^\\\"]*\\\"|[^,\\s]+\", args_str):\n",
        "            arg = arg.strip(\"'\\\"\")\n",
        "            try:\n",
        "                arg = int(arg) if arg.isdigit() else arg\n",
        "            except ValueError:\n",
        "                pass\n",
        "            args.append(arg)\n",
        "\n",
        "        if func_name in FUNCTION_MAPPING:\n",
        "            return FUNCTION_MAPPING[func_name](*args)\n",
        "        else:\n",
        "            return f\"Unsupported function: {func_name}\"\n",
        "\n",
        "    except Exception as e:\n",
        "        return f\"Error executing function: {str(e)}\"\n",
        "\n",
        "def evaluate_model(model, tokenizer, test_cases):\n",
        "    results = []\n",
        "    for case in test_cases:\n",
        "        function_call = generate_function_call(model, tokenizer, case['input'])\n",
        "        expected_call = case['output']\n",
        "\n",
        "        result = {\n",
        "            'input': case['input'],\n",
        "            'generated_call': function_call,\n",
        "            'expected_call': expected_call,\n",
        "            'call_match': function_call == expected_call\n",
        "        }\n",
        "\n",
        "        if result['call_match']:\n",
        "            try:\n",
        "                result['execution_result'] = execute_function_call(function_call)\n",
        "            except Exception as e:\n",
        "                result['execution_result'] = str(e)\n",
        "\n",
        "        results.append(result)\n",
        "\n",
        "    return results"
      ],
      "metadata": {
        "id": "ZikRxjR1p7Ia"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model setup\n",
        "model_name = \"mistralai/Mistral-7B-v0.1\"\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    device_map=\"auto\",\n",
        "    load_in_8bit=True,\n",
        "    torch_dtype=torch.float16\n",
        ")\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86,
          "referenced_widgets": [
            "50e96932174d40238701e042202bd9be",
            "d845fe2d937a41439f5a76a8afced2bd",
            "151c9bef718c496495278046fef0da42",
            "a33783aa24904bfaba4275a8e9a14310",
            "2448456c676149fca6c4d0a2b6bb6dd1",
            "90a51a28dd8d43e5bbc24fe135433473",
            "e7afced176dd4aa1937146fc966ba8d2",
            "475917b794cc4ba588f0f3d59c22ef4c",
            "524dc35d9ea24c67a58b55c731503631",
            "8e39d4d42d7b49309865152aac3560ed",
            "47bc2599a761449e9967a795a448eefb"
          ]
        },
        "id": "9LuSMAXtp9pS",
        "outputId": "9ac8a64c-298d-4f61-fb6c-d861edf49a66"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "50e96932174d40238701e042202bd9be"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Run evaluation\n",
        "evaluation_results = evaluate_model(model, tokenizer, test_cases)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xinh49aFp-qL",
        "outputId": "d17205d3-ce31-49fa-8a97-474067f74888"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:2097: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cpu, whereas the model is on cuda. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cuda') before running `.generate()`.\n",
            "  warnings.warn(\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Print results\n",
        "for result in evaluation_results:\n",
        "    print(f\"Input: {result['input']}\")\n",
        "    print(f\"Generated Call: {result['generated_call']}\")\n",
        "    print(f\"Expected Call: {result['expected_call']}\")\n",
        "    print(f\"Call Match: {result['call_match']}\")\n",
        "    if result.get('execution_result'):\n",
        "        print(f\"Execution Result: {result['execution_result']}\")\n",
        "    print(\"---\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J0Ua1vSWp_u7",
        "outputId": "e158c11f-7280-41ed-cd8d-e023da62823d"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input: What is the sum of 1 and 2?\n",
            "Generated Call: add(1, 2)\n",
            "Expected Call: add(1, 2)\n",
            "Call Match: True\n",
            "Execution Result: 3\n",
            "---\n",
            "Input: What is the sum of 7 and 3?\n",
            "Generated Call: add(7, 3)\n",
            "Expected Call: add(7, 3)\n",
            "Call Match: True\n",
            "Execution Result: 10\n",
            "---\n",
            "Input: Add 10 and 15\n",
            "Generated Call: add(10, 15)\n",
            "Expected Call: add(10, 15)\n",
            "Call Match: True\n",
            "Execution Result: 25\n",
            "---\n",
            "Input: How much is 20 plus 22?\n",
            "Generated Call: None\n",
            "Expected Call: add(20, 22)\n",
            "Call Match: False\n",
            "---\n",
            "Input: Add 50 and 30\n",
            "Generated Call: add(50, 30)\n",
            "Expected Call: add(50, 30)\n",
            "Call Match: True\n",
            "Execution Result: 80\n",
            "---\n",
            "Input: What is 18 plus 27?\n",
            "Generated Call: add(18, 27)\n",
            "Expected Call: add(18, 27)\n",
            "Call Match: True\n",
            "Execution Result: 45\n",
            "---\n",
            "Input: Calculate the square of 5\n",
            "Generated Call: square(5)\n",
            "Expected Call: square(5)\n",
            "Call Match: True\n",
            "Execution Result: 25\n",
            "---\n",
            "Input: Square of 6\n",
            "Generated Call: square(6)\n",
            "Expected Call: square(6)\n",
            "Call Match: True\n",
            "Execution Result: 36\n",
            "---\n",
            "Input: Square 7\n",
            "Generated Call: square(7)\n",
            "Expected Call: square(7)\n",
            "Call Match: True\n",
            "Execution Result: 49\n",
            "---\n",
            "Input: Square of 4\n",
            "Generated Call: square(4)\n",
            "Expected Call: square(4)\n",
            "Call Match: True\n",
            "Execution Result: 16\n",
            "---\n",
            "Input: Square of 8\n",
            "Generated Call: square(8)\n",
            "Expected Call: square(8)\n",
            "Call Match: True\n",
            "Execution Result: 64\n",
            "---\n",
            "Input: What's the cube of 2?\n",
            "Generated Call: cube(2)\n",
            "Expected Call: cube(2)\n",
            "Call Match: True\n",
            "Execution Result: 8\n",
            "---\n",
            "Input: Cube the number 4\n",
            "Generated Call: cube(4)\n",
            "Expected Call: cube(4)\n",
            "Call Match: True\n",
            "Execution Result: 64\n",
            "---\n",
            "Input: Cube of 3\n",
            "Generated Call: cube(3)\n",
            "Expected Call: cube(3)\n",
            "Call Match: True\n",
            "Execution Result: 27\n",
            "---\n",
            "Input: Cube 5\n",
            "Generated Call: cube(5)\n",
            "Expected Call: cube(5)\n",
            "Call Match: True\n",
            "Execution Result: 125\n",
            "---\n",
            "Input: Cube of 6\n",
            "Generated Call: cube(6)\n",
            "Expected Call: cube(6)\n",
            "Call Match: True\n",
            "Execution Result: 216\n",
            "---\n",
            "Input: Greet John\n",
            "Generated Call: greet('John')\n",
            "Expected Call: greet('John')\n",
            "Call Match: True\n",
            "Execution Result: Hello, John!\n",
            "---\n",
            "Input: Say hello to Sarah\n",
            "Generated Call: greet('Sarah')\n",
            "Expected Call: greet('Sarah')\n",
            "Call Match: True\n",
            "Execution Result: Hello, Sarah!\n",
            "---\n",
            "Input: Greet Emily\n",
            "Generated Call: greet('Emily')\n",
            "Expected Call: greet('Emily')\n",
            "Call Match: True\n",
            "Execution Result: Hello, Emily!\n",
            "---\n",
            "Input: Greet Michael\n",
            "Generated Call: greet('Michael')\n",
            "Expected Call: greet('Michael')\n",
            "Call Match: True\n",
            "Execution Result: Hello, Michael!\n",
            "---\n",
            "Input: Add the square of 3 to 10\n",
            "Generated Call: add(square(3), 10)\n",
            "Expected Call: add(square(3), 10)\n",
            "Call Match: True\n",
            "Execution Result: Error executing function: add() missing 1 required positional argument: 'b'\n",
            "---\n",
            "Input: Greet the person who got the cube of 2\n",
            "Generated Call: greet(cube(2))\n",
            "Expected Call: greet('cube(2)')\n",
            "Call Match: False\n",
            "---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VDtYzQfnug1m"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}