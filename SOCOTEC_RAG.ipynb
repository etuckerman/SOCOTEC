{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "authorship_tag": "ABX9TyMjo0rkbkNek93LFsWff3a8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "67db5aa646984588a44fa401fed55c2a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_633a5b2411cd4ee0b76879a507faea0d",
              "IPY_MODEL_a189a5297eaf40009e96a0a1646102b7",
              "IPY_MODEL_d2e7553a782a476183620f4b4a164577"
            ],
            "layout": "IPY_MODEL_42c971136e5c49c0af15605abe0572cb"
          }
        },
        "633a5b2411cd4ee0b76879a507faea0d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fcdd969c021642b69e3e25fe667da797",
            "placeholder": "​",
            "style": "IPY_MODEL_d03176b6c74d4ed7a2b327c976721246",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "a189a5297eaf40009e96a0a1646102b7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8fca27ca869246d3ad165c5f203d009f",
            "max": 4,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_7aa54f45cb5e4bcf92c1ff9bf0c37dc7",
            "value": 4
          }
        },
        "d2e7553a782a476183620f4b4a164577": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_43148a91d4d441a99261a0541485f35a",
            "placeholder": "​",
            "style": "IPY_MODEL_4b7792fcd80e4aad9f8188bf01cf1e2f",
            "value": " 4/4 [00:05&lt;00:00,  1.48s/it]"
          }
        },
        "42c971136e5c49c0af15605abe0572cb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fcdd969c021642b69e3e25fe667da797": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d03176b6c74d4ed7a2b327c976721246": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8fca27ca869246d3ad165c5f203d009f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7aa54f45cb5e4bcf92c1ff9bf0c37dc7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "43148a91d4d441a99261a0541485f35a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4b7792fcd80e4aad9f8188bf01cf1e2f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/etuckerman/SOCOTEC/blob/main/SOCOTEC_RAG.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "else:\n",
        "    print(\"No GPU found!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O2bJfu0aKH5j",
        "outputId": "0c82997e-c9f8-4cf9-8136-f4d4017dac9c"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU: NVIDIA A100-SXM4-40GB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# Enable mixed precision for faster computations on A100\n",
        "torch.set_default_dtype(torch.float16)\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "torch.backends.cudnn.benchmark = True\n"
      ],
      "metadata": {
        "id": "xMuONxxvKSQs"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install llama_parse huggingface_hub langchain chromadb nest_asyncio langchain-community unstructured langchain-huggingface gradio"
      ],
      "metadata": {
        "id": "gTJOqMDU0T4i"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p6gpqv8cKTtY",
        "outputId": "d4e10127-8005-4d23-dd6a-a9e974bfc300"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tue Jan  7 17:56:43 2025       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  NVIDIA A100-SXM4-40GB          Off | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   31C    P0              43W / 400W |      5MiB / 40960MiB |      0%      Default |\n",
            "|                                         |                      |             Disabled |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "|  No running processes found                                                           |\n",
            "+---------------------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# RAG PIPELINE"
      ],
      "metadata": {
        "id": "LJiM1qxsJ1o7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loading and Preprocessing"
      ],
      "metadata": {
        "id": "slq5koVXKV3o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nest_asyncio\n",
        "from llama_parse import LlamaParse\n",
        "\n",
        "# Apply nest_asyncio to handle the event loop\n",
        "nest_asyncio.apply()\n",
        "\n",
        "### BASIC PARSING\n",
        "# # Initialize the LlamaParse parser with optimized parsing instructions\n",
        "# parser = LlamaParse(\n",
        "#     api_key=\"llx-ZTieolOu9t8Ks9FvurLVGbBujjpap5s63nI0PHXsv4EV4szb\",\n",
        "#     result_type=\"markdown\",  # Retain markdown format for structured output\n",
        "#     language=\"en\",  # Set to English since the IBC is in English\n",
        "#     verbose=True,  # Enable detailed logs to monitor parsing performance\n",
        "#     is_formatting_instruction=True,  # Preserve formatting for context retrieval\n",
        "#     parsing_instruction=\"\"\"\n",
        "#         Extract the following key elements from the document:\n",
        "#         1. Chapter titles and their numbers.\n",
        "#         2. Section headings and subheadings with their corresponding numbers.\n",
        "#         3. Key definitions and terms listed in the document.\n",
        "#         4. Detailed descriptions of occupancy classifications, fire-resistance requirements, and structural design criteria.\n",
        "#         5. All tables and their captions, including their associated data.\n",
        "#         6. Any reference codes, figures, or diagrams mentioned in the text.\n",
        "#         Format the extracted data in a structured and readable manner, preserving markdown styling for clarity (e.g., **bold** headings, bullet points for lists, etc.).\n",
        "#     \"\"\"\n",
        "# )\n",
        "\n",
        "### OPTIMISED PARSING TEST [currently costs 30$ so i cancelled it]\n",
        "# Initialize the LlamaParse parser with optimized parameters\n",
        "parser = LlamaParse(\n",
        "    api_key=\"llx-ZTieolOu9t8Ks9FvurLVGbBujjpap5s63nI0PHXsv4EV4szb\",\n",
        "    is_remote=False,  # Processing locally for faster iterations\n",
        "    verbose=True,  # Keep verbose for detailed logs\n",
        "    show_progress=True,  # Show progress for better tracking\n",
        "    language=\"en\",  # Document language is English\n",
        "    split_by_page=True,  # Process document page by page for modularity\n",
        "    result_type=\"markdown\",  # Export as markdown for better structuring\n",
        "    max_timeout=3000,  # Increase timeout for processing large documents\n",
        "    num_workers=6,  # Utilize 6 workers for concurrent processing\n",
        "    parsing_instruction=(\n",
        "        \"Extract all critical information, including definitions, tables, figures, and important text \"\n",
        "        \"relevant to occupancy classifications, construction types, fire-resistance requirements, \"\n",
        "        \"design loads, and any other regulations. Focus on sections that may aid in answering queries.\"\n",
        "    ),\n",
        "    structured_output=False,  # Output as plain markdown, structured parsing is unnecessary here\n",
        "    annotate_links=True,  # Annotate links for better context during retrieval\n",
        "    auto_mode=True,  # Enable auto mode to trigger optimizations for certain elements\n",
        "    auto_mode_trigger_on_table_in_page=True,  # Prioritize tables (highly structured info)\n",
        "    auto_mode_trigger_on_image_in_page=True,  # Include charts/diagrams for completeness\n",
        "    disable_ocr=False,  # Allow OCR for text in non-standard formats\n",
        "    extract_charts=True,  # Include chart data in the parsed output\n",
        "    extract_layout=False,  # Skip layout info, focusing purely on content\n",
        "    premium_mode=True,  # Enable premium processing for improved accuracy\n",
        "    page_separator=\"\\n\\n---\\n\\n\",  # Separate pages clearly for retrieval\n",
        "    max_pages=None,  # Process the entire document\n",
        "    continuous_mode=False,  # Avoid continuous mode; keep pages distinct\n",
        ")\n",
        "\n",
        "\n",
        "# Parse the syllabus document\n",
        "parsed_documents = parser.load_data(\"/content/IBC.pdf\")\n",
        "\n",
        "# Save the parsed results to a markdown or any preferred format\n",
        "with open('IBC.md', 'w') as f:\n",
        "    for doc in parsed_documents:\n",
        "        f.write(doc.text + '\\n')\n"
      ],
      "metadata": {
        "id": "vkLj9PSOKadB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "outputId": "ecb1e49c-b481-4833-cd01-7ba6dfd1b82b"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Started parsing the file under job_id 09ab8e9f-7e24-47a3-a891-9971481e4ae3\n",
            "....."
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-01ae829b1e46>\u001b[0m in \u001b[0;36m<cell line: 59>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;31m# Parse the syllabus document\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m \u001b[0mparsed_documents\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/IBC.pdf\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;31m# Save the parsed results to a markdown or any preferred format\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/llama_parse/base.py\u001b[0m in \u001b[0;36mload_data\u001b[0;34m(self, file_path, extra_info, fs)\u001b[0m\n\u001b[1;32m    739\u001b[0m         \u001b[0;34m\"\"\"Load data from the input path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    740\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 741\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0masyncio_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextra_info\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    742\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mRuntimeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    743\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mnest_asyncio_err\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/llama_index/core/async_utils.py\u001b[0m in \u001b[0;36masyncio_run\u001b[0;34m(coro)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0;31m# If we're here, there's an existing loop but it's not running\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mloop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_until_complete\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcoro\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mRuntimeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nest_asyncio.py\u001b[0m in \u001b[0;36mrun_until_complete\u001b[0;34m(self, future)\u001b[0m\n\u001b[1;32m     90\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_log_destroy_pending\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_once\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stopping\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m                     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nest_asyncio.py\u001b[0m in \u001b[0;36m_run_once\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    113\u001b[0m                 scheduled[0]._when - self.time(), 0), 86400) if scheduled\n\u001b[1;32m    114\u001b[0m             else None)\n\u001b[0;32m--> 115\u001b[0;31m         \u001b[0mevent_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_selector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_events\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevent_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/selectors.py\u001b[0m in \u001b[0;36mselect\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    467\u001b[0m             \u001b[0mready\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    468\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 469\u001b[0;31m                 \u001b[0mfd_event_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_selector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpoll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_ev\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    470\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mInterruptedError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    471\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Embedding and Vector Store setup"
      ],
      "metadata": {
        "id": "PxvoL9Ir3mLq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "When processing such a substantial document for a Retrieval-Augmented Generation (RAG) system, it's crucial to optimize the text chunking and embedding process to balance performance and accuracy.\n",
        "\n",
        "Optimizing Text Chunking and Embedding:\n",
        "\n",
        "Text Chunking:\n",
        "\n",
        "Chunk Size: Given the document's length, consider setting the chunk_size to 1500 characters. This size is manageable for most language models and ensures that each chunk contains sufficient context.\n",
        "Overlap: Maintain an overlap of 100 characters (chunk_overlap=100). This overlap helps preserve context between chunks, which is beneficial for understanding references across sections.\n",
        "Embeddings:\n",
        "\n",
        "Model Selection: The all-MiniLM-L6-v2 model is efficient and effective for generating embeddings. It's a good choice for balancing performance and computational efficiency.\n",
        "Vector Store: Utilize Chroma as the vector store. It's optimized for handling large datasets and supports efficient similarity searches."
      ],
      "metadata": {
        "id": "QmntSvKXLUUx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain.document_loaders import UnstructuredMarkdownLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "\n",
        "# Load the parsed markdown document\n",
        "loader = UnstructuredMarkdownLoader(\"IBC.md\")\n",
        "docs = loader.load()\n",
        "\n",
        "# Split documents into chunks\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1500, chunk_overlap=100)\n",
        "texts = text_splitter.split_documents(docs)\n",
        "\n",
        "# Create embeddings and vector store\n",
        "embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
        "vectorstore = Chroma.from_documents(texts, embeddings)\n",
        "retriever = vectorstore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 2})\n"
      ],
      "metadata": {
        "id": "VHSYUflM0wMi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e593c287-253a-44f9-fab6-33632ef9bcad"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MODEL SETUP"
      ],
      "metadata": {
        "id": "IqgH0q183ofW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3: Load the Qwen Model\n",
        "from transformers import pipeline\n",
        "from langchain_huggingface import HuggingFacePipeline\n",
        "\n",
        "qwen_pipe = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=\"Qwen/Qwen2.5-7B\",\n",
        "    tokenizer=\"Qwen/Qwen2.5-7B\",\n",
        "    device=0  # Use GPU\n",
        ")\n",
        "qwen_llm = HuggingFacePipeline(pipeline=qwen_pipe)"
      ],
      "metadata": {
        "id": "YzC9ZND03Bm9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67,
          "referenced_widgets": [
            "67db5aa646984588a44fa401fed55c2a",
            "633a5b2411cd4ee0b76879a507faea0d",
            "a189a5297eaf40009e96a0a1646102b7",
            "d2e7553a782a476183620f4b4a164577",
            "42c971136e5c49c0af15605abe0572cb",
            "fcdd969c021642b69e3e25fe667da797",
            "d03176b6c74d4ed7a2b327c976721246",
            "8fca27ca869246d3ad165c5f203d009f",
            "7aa54f45cb5e4bcf92c1ff9bf0c37dc7",
            "43148a91d4d441a99261a0541485f35a",
            "4b7792fcd80e4aad9f8188bf01cf1e2f"
          ]
        },
        "outputId": "796a3a05-d4f1-4996-cae3-04be4f7620a9"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "67db5aa646984588a44fa401fed55c2a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Refine Prompt Template"
      ],
      "metadata": {
        "id": "eYNre8Oq3s8t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "prompt = PromptTemplate(\n",
        "    input_variables=[\"context\", \"query\"],\n",
        "    template=(\n",
        "        \"You are Qwen, created by Alibaba Cloud. You are a helpful assistant.\"\n",
        "        \"You have the knowledge of the IBC 2018 International Building Code book.\"\n",
        "        \"You use this knowledge to answer queries to users, don't reference the document in third person, just speak as if you know the information.\"\n",
        "        \"Given the following context, provide a concise answer to the query.:\\n\\n\"\n",
        "        \"{context}\\n\\n\"\n",
        "        \"Query: {query}\\n\"\n",
        "        \"Response:\"\n",
        "    ),\n",
        ")"
      ],
      "metadata": {
        "id": "AxaocsC83FoV"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup RetrivalQA Chain"
      ],
      "metadata": {
        "id": "OSmpW7Fi3vlc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# # Step 6: Test the RAG System\n",
        "# query_1 = \"What is the purpose of Appendix B: Board of Appeals?\"\n",
        "# response_1 = qa_chain.invoke({\"query\": query_1})\n",
        "# print(f\"Answer 1: {response_1}\")\n"
      ],
      "metadata": {
        "id": "r2LMo29J67OK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# query_2 = \"Explain the key concepts discussed in the document?\"\n",
        "# response_2 = qa_chain.invoke({\"query\": query_2})\n",
        "# print(f\"Answer 2: {response_2}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5xKnfBLZ1ka-",
        "outputId": "df80d4f2-ea12-4aa3-a2f4-352ad0e434f5"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer 2: {'query': 'Explain the key concepts discussed in the document?', 'result': \"Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\\n\\nContext:\\nChapter 29 of IBC correlates with Chapters 3 & 4 of IPC for plumbing fixtures and facilities\\n\\nThe image also provides brief descriptions of Chapters 1 and 2 of the IBC:\\n\\nChapter 1 establishes the scope, applicability, and administration of the code.\\n\\nChapter 2 contains definitions of terms used throughout the code.\\n\\nThe document emphasizes the importance of every word, term, and punctuation mark in the code, as they can impact the meaning and intended results of the code provisions.\\n\\nmeaning in the code and the code meaning can differ substantially from the ordinarily understood meaning of the term as used outside of the code. Where understanding of a term's definition is especially key to or necessary for understanding a particular code provision, the term is shown in italics wherever it appears in the code.\\n\\nThe user of the code should be familiar with and consult this chapter because the definitions are essential to the correct interpretation of the code. Where a term is not defined, such terms shall have the ordinarily accepted meaning.\\n\\nContext:\\nAn arrow (➳) indicates where an entire section, paragraph, exception, or table has been deleted.\\n\\nA single asterisk (*) indicates text or table relocation within the code.\\n\\nA double asterisk (**) indicates text or table relocation from elsewhere in the code.\\n\\nRelocation Table: The image includes a table showing the new locations of various sections in the 2018 edition and their corresponding locations in the 2015 edition.\\n\\nCoordination of the International Codes:\\n\\nEmphasizes the strength of technical provision coordination in the ICC family of model codes.\\n\\nCodes can be used as a complete set or as stand-alone documents.\\n\\nSome technical provisions are duplicated across codes for flexibility and completeness.\\n\\nItalicized Terms:\\n\\nWords defined in Chapter 2 (Definitions) are italicized in the code text where that specific definition applies.\\n\\nNon-italicized words use common definitions.\\n\\nUsers should pay attention to italicized terms for code-specific meanings.\\n\\nIn Sections 1903 through 1905, italics indicate provisions differing from ACI 318.\\n\\nThis information is crucial for users of the International Building Code to understand how to interpret changes, relocations, and specific terminology in the 2018 edition.\\n\\nAdoption\\n\\nQuestion: Explain the key concepts discussed in the document?\\nHelpful Answer: The document discusses several key concepts related to the International Building Code (IBC) and its relationship with the International Plumbing Code (IPC). Here are the main points:\\n\\n1. **Correlation Between Codes**: Chapter 29 of the IBC correlates with Chapters 3 & 4 of the IPC for plumbing fixtures and facilities. This means that these chapters in both codes cover similar topics, specifically plumbing systems and related facilities.\\n\\n2. **Scope and Administration**: Chapter 1 of the IBC establishes the scope, applicability, and administration of the code. This includes defining who the code applies to and how it should be enforced.\\n\\n3. **Definitions**: Chapter 2 of the IBC contains definitions of terms used throughout the code. These definitions are crucial for understanding the code correctly, as the meaning of terms within the code may differ from their ordinary usage.\\n\\n4. **Italicized Terms**: Words defined in Chapter 2 are italicized in the code text where that specific definition applies. This helps users understand the code-specific meanings of these terms.\\n\\n5. **Relocation of Text and Tables**: The document explains that an arrow (➳) indicates where an entire section, paragraph, exception, or table has been deleted, while a single asterisk (*) indicates text or table relocation within the code. A double asterisk (**) indicates text or table relocation from elsewhere in the code. A relocation table is provided to show the new locations of various sections in the 2018 edition and their corresponding locations in the 2015 edition.\\n\\n6. **Coordination of the International Codes**: The document emphasizes the strength of technical provision coordination in the ICC family of model codes. Codes can be used as a complete set or as stand-alone documents, and some technical provisions are duplicated across codes for flexibility and completeness.\\n\\n7. **Adoption**: The document discusses the adoption of the IBC and its relationship with other codes, including the IPC. It highlights the importance of understanding the code-specific meanings of terms and the relocation of text and tables.\\n\\nThese key concepts provide a comprehensive overview of the IBC and its relationship with other codes, as well as the importance of understanding the code-specific meanings of terms and the relocation of text and tables.\"}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # Example IBC-specific questions\n",
        "# queries = [\n",
        "#     \"What is the purpose of Appendix B: Board of Appeals?\",\n",
        "#     \"What are the occupancy classifications defined in Chapter 3?\",\n",
        "#     \"How does the IBC define mixed-use occupancies?\",\n",
        "#     \"What are the fire-resistance requirements for Type I construction?\",\n",
        "#     \"What are the minimum design loads for buildings and structures?\"\n",
        "# ]\n"
      ],
      "metadata": {
        "id": "Vs0ge-qAJ14-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Loop through and retrieve answers\n",
        "# for query in queries:\n",
        "#     response = qa_chain.invoke({\"query\": query})\n",
        "#     print(f\"Query: {query}\\nAnswer: {response}\\n\")\n"
      ],
      "metadata": {
        "id": "leFCejDPzB7z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import RetrievalQA\n",
        "\n",
        "qa_chain = RetrievalQA.from_llm(llm=qwen_llm, retriever=retriever)\n"
      ],
      "metadata": {
        "id": "TEjyytd_3JG3"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Gradio implementation\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "uSyKjJj86Ftn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "import torch\n",
        "from langchain.chains import RetrievalQA\n",
        "\n",
        "# Initialize the model, retriever, and other necessary components here (same as in your original code)\n",
        "# Make sure to have the model and the retriever set up before proceeding with the Gradio interface.\n",
        "\n",
        "# Define the function that will display the results in a more readable format\n",
        "def query_rag_system(query):\n",
        "    # Use the qa_chain.invoke to get the response for the query\n",
        "    response = qa_chain.invoke({\"query\": query})\n",
        "    # Return the response in a user-friendly format (you can format it as needed)\n",
        "    return response.get('result', \"No result found\")\n",
        "\n",
        "# Create a Gradio interface\n",
        "interface = gr.Interface(\n",
        "    fn=query_rag_system,  # This is the function that will be called to generate the output\n",
        "    inputs=gr.Textbox(label=\"Enter your query\"),  # The input for the user query\n",
        "    outputs=gr.Textbox(label=\"RAG System Answer\", lines=10),  # The output for displaying the result\n",
        "    live=True,  # Optional: Allows for live updates as the user types\n",
        "    title=\"RAG Query Interface\",  # Title for the interface\n",
        "    description=\"Enter a query related to the IBC 2018 International Building Code, and the system will provide an answer based on the context.\"\n",
        ")\n",
        "\n",
        "# Launch the interface\n",
        "interface.launch(debug=True, share=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 650
        },
        "id": "NMQMecVw6Hmp",
        "outputId": "119a714a-bfb4-43dc-af3b-c0c1515207b4"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://46914f41892dbc8752.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://46914f41892dbc8752.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Keyboard interruption in main thread... closing server.\n",
            "Killing tunnel 127.0.0.1:7861 <> https://46914f41892dbc8752.gradio.live\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "q1Tyrb9Q6IHu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}